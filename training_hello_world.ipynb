{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aitextgen Training Hello World\n",
    "\n",
    "_Last Updated: Feb 21, 2021 (v.0.4.0)_\n",
    "\n",
    "by Max Woolf\n",
    "\n",
    "A \"Hello World\" Tutorial to show how training works with aitextgen, even on a CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_TPU_AVAILABLE' from 'pytorch_lightning.utilities' (/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/utilities/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maitextgen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mTokenDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m TokenDataset\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maitextgen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenizers\u001b[39;00m \u001b[39mimport\u001b[39;00m train_tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maitextgen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2ConfigCPU\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maitextgen\u001b[39;00m \u001b[39mimport\u001b[39;00m aitextgen\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/aitextgen/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maitextgen\u001b[39;00m \u001b[39mimport\u001b[39;00m aitextgen  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/aitextgen/aitextgen.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m create_gdrive_folder\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mTokenDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m TokenDataset\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m ATGProgressBar, ATGTransformer\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     download_gpt2,\n\u001b[1;32m     34\u001b[0m     find_index_of_subset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     set_seed,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39maitextgen\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/aitextgen/train.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprogress\u001b[39;00m \u001b[39mimport\u001b[39;00m ProgressBar\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m _TPU_AVAILABLE\n\u001b[1;32m     17\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mATGTransformer\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    A training module for aitextgen.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_TPU_AVAILABLE' from 'pytorch_lightning.utilities' (/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/utilities/__init__.py)"
     ]
    }
   ],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "import os, os.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download this [text file of Shakespeare's plays](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt), to the folder with this notebook, then put the name of the downloaded Shakespeare text for training into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"number_theory.json\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train a custom Byte Pair Encoding Tokenizer on the downloaded text!\n",
    "\n",
    "This will save one file: `aitextgen.tokenizer.json`, which contains the information needed to rebuild the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer(file_name)\n",
    "tokenizer_file = 'aitextgen.tokenizer.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GPT2ConfigCPU()` is a mini variant of GPT-2 optimized for CPU-training.\n",
    "\n",
    "e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2. This dramatically speeds training up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2ConfigCPU()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate aitextgen using the created tokenizer and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ai = aitextgen(tokenizer_file=tokenizer_file, config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build datasets for training by creating TokenDatasets, which automatically processes the dataset with the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 86712.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenDataset containing 462,820 subsets loaded from file at input.txt."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TokenDataset(file_name, tokenizer_file=tokenizer_file, block_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder. On a 2020 8-core iMac, this took ~25 minutes to run.\n",
    "\n",
    "The configuration below processes 400,000 subsets of tokens (8 * 50000), which is about just one pass through all the data (1 epoch). Ideally you'll want multiple passes through the data and a training loss less than `2.0` for coherent output; when training a model from scratch, that's more difficult, but with long enough training you can get there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "'s dead;\n",
      "But is no winted in his northeritiff\n",
      "Tave passage, and eleve your hours.\n",
      "\n",
      "PETRUCHIO:\n",
      "What is this I does, I will, sir;\n",
      "That, you have, nor tolding we\n",
      "==========\n",
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I know, to, fair beat, to my soul is wonder'd intend.\n",
      "\n",
      "KING RICHARD III:\n",
      "Hold, and threaten, my lord, and my shame!\n",
      "\n",
      "QUEEN ELIZAB\n",
      "==========\n",
      "\u001b[1m15,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m15,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "s of capitcts!\n",
      "\n",
      "EDWARD:\n",
      "Gardener, what is this hour will not say.\n",
      "What, shall the joint, I pray, if they\n",
      "Harry, let bid me as he would readness so.\n",
      "\n",
      "B\n",
      "==========\n",
      "\u001b[1m20,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m20,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " for.\n",
      "\n",
      "ROMEO:\n",
      "Fair to the iercing wide's fretch,\n",
      "And happy talk of the master,\n",
      "And waste their justice with the feet and punning,\n",
      "And therefore be ben\n",
      "==========\n",
      "\u001b[1m25,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m25,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ",\n",
      "That we we will have not lose such.\n",
      "\n",
      "See, to the kingdom of our virtue,\n",
      "You banish'd our purpose, for our own ignorse,\n",
      "Dispon I remain, and seem'd in\n",
      "==========\n",
      "\u001b[1m30,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m30,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".\n",
      "\n",
      "BENVOLIO:\n",
      "O, she's dead!\n",
      "\n",
      "CAMILLO:\n",
      "No, my lord;\n",
      "These accession will be hous.\n",
      "\n",
      "DERBY:\n",
      "No, my lord.\n",
      "\n",
      "GLOUCESTER:\n",
      "What is the\n",
      "==========\n",
      "\u001b[1m35,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m35,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ",\n",
      "And whiles it is but the castle,\n",
      "That stavin'd in the gods of men.\n",
      "\n",
      "COMFEY:\n",
      "What, then?\n",
      "\n",
      "ELBOW:\n",
      "Peace, my lord,\n",
      "And weat your greats\n",
      "==========\n",
      "\u001b[1m40,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m40,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\n",
      "The white mercy of the sun upon my past,\n",
      "Of my father's son be first, thy sake,\n",
      "His son's chief son, and my includy;\n",
      "And if thy brother's loss, thy thrief,\n",
      "\n",
      "==========\n",
      "\u001b[1m45,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m45,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " to the crown,\n",
      "Or I'll privy I have.\n",
      "\n",
      "POLIXENES:\n",
      "I have been a stir.\n",
      "\n",
      "LEONTES:\n",
      "The worshiped, the benefition of the crown.\n",
      "\n",
      "His somet\n",
      "==========\n",
      "\u001b[1m50,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m50,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ":\n",
      "Catesby, girls, and make avoides;\n",
      "But, welcome a far\n",
      "That ever home, like a villain, and behold\n",
      "Canusy not passing nonquial at the g\n",
      "==========\n",
      "Loss: 2.940 — Avg: 2.884: 100%|██████████| 50000/50000 [31:39<00:00, 26.32it/s]\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8, num_steps=5000, generate_every=5000, save_every=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate text from your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "Abook, ho! forthing me, gentle Earl's royal king,\n",
      "And this, I, with that I do not beseech you\n",
      "To visit the battle, that I should believe you,\n",
      "Which I would never\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Confound is gone, thou art a maid into the widow;\n",
      "Put up my life and make me no harmony\n",
      "And make thee I know uncle,\n",
      "Unconted and curses: therefore in my\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "God push! but what days to see\n",
      "The giving bleedom's heart I do? Therefore,\n",
      "And most unless I had rather. He saddle\n",
      "Take your cold shack down; and so far I\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Persetain'd up the earth of mercy,\n",
      "And never yet, the sun to make him all the\n",
      "More than my battle.\n",
      "\n",
      "ROMEO:\n",
      "I warrant him, to know, we'll not do't, but hate me\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Methinks I am a mile, and trench one\n",
      "Thy winded makes, in faults and cast\n",
      "With one to meether, of twenty days,\n",
      "That in my waters, that f\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "O, here is such a woman guilty.\n",
      "\n",
      "ROMEO:\n",
      "I do not think it; I should be renowned\n",
      "That I am in that which can controy\n",
      "A bawd I take it to the purpose.\n",
      "\n",
      "JU\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I know not what I am.\n",
      "\n",
      "FLORIZEL:\n",
      "Ay, as I did,\n",
      "I would be adverpite of the homely treason\n",
      "From the doubled in the farm of his bed.\n",
      "Ta\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I pray you, he would have taken to him but,\n",
      "And freely mark his into a fine of it,\n",
      "Speak to the second to our cheek;\n",
      "And every day, and sanctious cover\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I had left me--born to be drawn.\n",
      "\n",
      "JULIET:\n",
      "My husbour, I will have thee here:\n",
      "And, I have found to seek thyself.\n",
      "\n",
      "JULIET:\n",
      "I will be not b\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "That is a hour,\n",
      "The castard is, I'll not buy, or indeeding.\n",
      "\n",
      "Nurse:\n",
      "LADY CAPULET:\n",
      "The matter, that ta'en as I may find thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your trained model, you can reload the model at any time by providing the `pytorch_model.bin` model weights, the `config`, and the `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai2 = aitextgen(model_folder=\"trained_model\",\n",
    "                tokenizer_file=\"aitextgen.tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "Boy, unreacher, unhallupony, in Padua,\n",
      "Untimely fall till I be learn'd.\n",
      "\n",
      "ROMEO:\n",
      "Fie, good friar, be quick, for I am,\n",
      "I'll\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I'll be plain, I am a tail of blessed wounds;\n",
      "For I am dead, I have not borne to make\n",
      "A couple of her fortune, but that I'll bear,\n",
      "And say 'Ay, chur\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "And yet I am a resolution of my dear dear:\n",
      "If I have not reason to do me say\n",
      "I'll deny the sea of my body to answer,\n",
      "And all thy tale, or I have my m\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Intenty to a bawd of my bait,--\n",
      "\n",
      "JULIET:\n",
      "No, I hope to know the title,\n",
      "For that I wish her place.\n",
      "\n",
      "JULIET:\n",
      "Do I assure her?\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "O, what's the parle that I chide thee,\n",
      "That honourable may be, that I have still'd thee:\n",
      "I pray thee, my lord.\n",
      "\n",
      "MERCUTIO:\n",
      "I', my lord.\n",
      "\n",
      "ROMEO:\n",
      "Here is a\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "And, for I am, and not talk of that?\n",
      "\n",
      "ROMEO:\n",
      "Where's my child, I would guess thee here.\n",
      "\n",
      "JULIET:\n",
      "Nay, boy, I'll not be bowling why I;\n",
      "O thou\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "O, but thou hast seen thee of mine own.\n",
      "\n",
      "ROMEO:\n",
      "I would assist thee--\n",
      "\n",
      "JULIET:\n",
      "Ay, it is, and not so.\n",
      "\n",
      "ROMEO:\n",
      "No, but that I must told me with it.\n",
      "\n",
      "ROMEO\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "No, no, nor I am. I am content.\n",
      "\n",
      "BENVOLIO:\n",
      "I will not, sir: but I have required\n",
      "As I am grown in the lawful virtue\n",
      "That it hath bid you think, and I\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "That I should pardon, I would be gone.\n",
      "\n",
      "ESCALUS:\n",
      "I should believe you, sir, sir, ay, I would not\n",
      "not know more, but that I can, but I would have savour me.\n",
      "\n",
      "P\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "And thou, I will find out thy life the wind of love.\n",
      "\n",
      "ROMEO:\n",
      "It is the morning groom of it.\n",
      "\n",
      "JULIET:\n",
      "Fie, good sweet boy, I will take my leave to a happy day,\n"
     ]
    }
   ],
   "source": [
    "ai2.generate(10, prompt=\"1+1 is\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT License\n",
    "\n",
    "Copyright (c) 2021 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
